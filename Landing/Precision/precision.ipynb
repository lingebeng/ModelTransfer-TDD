{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a908fbd4",
   "metadata": {},
   "source": [
    "### Learning Precision on JAX\n",
    "\n",
    "#### 1. XLA 编译的不确定性 \n",
    "(a + b) + c   \n",
    "a + (b + c)\n",
    "有时为了并行化，并不总是一样，可能会重排求和顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b247dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f30c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n",
      "Eager divergence tensor(7.8756e-07, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Compile divergence divergence tensor(8.9890e-07, dtype=torch.float64, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm([10000, 1000])  # LayerNorm for 2D input\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model().eval()\n",
    "\n",
    "x = torch.randn(\n",
    "    1, 3, 10000, 1000\n",
    ")  # As `H` and `W` increase, the error might be amplified\n",
    "\n",
    "inputs = [x]\n",
    "\n",
    "c_model = torch.compile(model)\n",
    "\n",
    "output = model(*inputs)\n",
    "\n",
    "c_output = c_model(*inputs)\n",
    "\n",
    "print(torch.allclose(output, c_output, 1e-5, 1e-5))  # loose check in fp32\n",
    "print(torch.max(torch.abs(output - c_output)))\n",
    "\n",
    "fp_64_ref = c_model(x.double())\n",
    "print(\"Eager divergence\", torch.max(torch.abs(output - fp_64_ref)))\n",
    "print(\"Compile divergence divergence\", torch.max(torch.abs(c_output - fp_64_ref)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342d381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n",
      "Eager divergence tensor(9.2591e-07, dtype=torch.float64, grad_fn=<MaxBackward1>)\n",
      "Compile divergence tensor(9.6588e-07, dtype=torch.float64, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm([10000, 1000])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model().eval()\n",
    "x = torch.randn(1, 3, 10000, 1000)\n",
    "inputs = [x]\n",
    "c_model = torch.compile(model)\n",
    "output = model(*inputs)\n",
    "c_output = c_model(*inputs)\n",
    "\n",
    "print(torch.allclose(output, c_output, 1e-5, 1e-5))  # loose check in fp32\n",
    "print(torch.max(torch.abs(output - c_output)))\n",
    "\n",
    "fp_64_ref = c_model(x.double())\n",
    "print(\"Eager divergence\", torch.max(torch.abs(output - fp_64_ref)))\n",
    "print(\"Compile divergence\", torch.max(torch.abs(c_output - fp_64_ref)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3525e530",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax.config'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m      3\u001b[39m config.update(\u001b[33m\"\u001b[39m\u001b[33mjax_enable_x64\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m a = jnp.array([\u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m), \u001b[32m0.0\u001b[39m, \u001b[32m1\u001b[39m, -\u001b[32m0.0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mnan\u001b[39m\u001b[33m'\u001b[39m), -\u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m), -\u001b[32m1\u001b[39m, -\u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mnan\u001b[39m\u001b[33m'\u001b[39m)]).astype(jnp.float32)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'jax.config'"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.config import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "a = jnp.array(\n",
    "    [float(\"inf\"), 0.0, 1, -0.0, float(\"nan\"), -float(\"inf\"), -1, -float(\"nan\")]\n",
    ").astype(jnp.float32)\n",
    "b = jnp.sort(a)\n",
    "print(a)  # [ inf   0.   1.  -0.  nan -inf  -1.  nan]\n",
    "print(b)  # [-inf  -1.   0.  -0.   1.  inf  nan  nan]\n",
    "\n",
    "a64 = a.astype(jnp.float64)\n",
    "print(a64)  # [ inf   0.   1.   0.  nan -inf  -1.  nan]\n",
    "\n",
    "b64 = jnp.sort(\n",
    "    a64\n",
    ")  # XlaRuntimeError: UNIMPLEMENTED: While rewriting computation to not contain X64 element types, XLA encountered an HLO for which this rewriting is not implemented: %bitcast-convert.9 = s64[] bitcast-convert(f64[] %Arg_0.3), metadata={op_name=\"jit(sort)/jit(main)/bitcast_convert_type[new_dtype=int64]\" source_file=\"<ipython-input-6-9289161c541b>\" source_line=1}\n",
    "print(b64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db0715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 灾难性抵消\n",
    "x = 1234567891.0000001\n",
    "y = 1234567891.0\n",
    "x - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7bb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e+16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 大数吃小数\n",
    "x = 1e16\n",
    "y = 1.0\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a2a1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4q/td31czvd29qc_qk6bh070n680000gn/T/ipykernel_17322/781482610.py:3: RuntimeWarning: overflow encountered in multiply\n",
      "  x * 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([inf], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 浮点数溢出\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([2e38], dtype=np.float32)\n",
    "x * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d34139c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: torch.bfloat16, Tolerances: (0.016, 1e-05)\n",
      "Precision: torch.float16, Tolerances: (0.001, 1e-05)\n",
      "Precision: torch.float32, Tolerances: (1.3e-06, 1e-05)\n",
      "Precision: torch.float64, Tolerances: (1e-07, 1e-07)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.testing._comparison import get_tolerances\n",
    "\n",
    "precision = [torch.bfloat16, torch.float16, torch.float32, torch.float64]\n",
    "\n",
    "for dtype in precision:\n",
    "    tolerance = get_tolerances(dtype, rtol=None, atol=None)\n",
    "    print(f\"Precision: {dtype}, Tolerances: {tolerance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f63ce421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "# 开启 JAX 的 float64 支持（必须在开头）\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# 生成统一的随机输入 (双精度)\n",
    "input_np = np.random.randn(2, 16, 128).astype(np.float64)\n",
    "gamma_np = np.ones((128,), dtype=np.float64)\n",
    "beta_np = np.zeros((128,), dtype=np.float64)\n",
    "eps = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e953e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pt = torch.from_numpy(input_np)\n",
    "gamma_pt = torch.from_numpy(gamma_np)\n",
    "beta_pt = torch.from_numpy(beta_np)\n",
    "\n",
    "# 使用原生算子进行计算\n",
    "# 注意：LayerNorm 的参数通常是 normalized_shape\n",
    "ln_pt = torch.nn.functional.layer_norm(\n",
    "    input_pt, (128,), weight=gamma_pt, bias=beta_pt, eps=eps\n",
    ")\n",
    "res_pt = ln_pt.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0574bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 强制 JAX 使用最高精度，避免 TPU/GPU 上的隐式 bf16 转换\n",
    "with jax.default_matmul_precision(\"highest\"):\n",
    "\n",
    "    def jax_layer_norm(x, g, b, epsilon):\n",
    "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
    "        # 使用有偏方差 (ddof=0)\n",
    "        var = jnp.var(x, axis=-1, keepdims=True, ddof=0)\n",
    "        return (x - mean) / jnp.sqrt(var + epsilon) * g + b\n",
    "\n",
    "    res_jax = jax_layer_norm(input_np, gamma_np, beta_np, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b295aa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 精度对齐通过！\n"
     ]
    }
   ],
   "source": [
    "# 使用 PyTorch 提供的标准测试工具\n",
    "res_jax_np = np.array(res_jax)\n",
    "res_jax_pt = torch.from_numpy(res_jax_np)\n",
    "try:\n",
    "    torch.testing.assert_close(\n",
    "        res_jax_pt,\n",
    "        torch.from_numpy(res_pt),\n",
    "        rtol=None,\n",
    "        atol=None,  # 强制使用官方默认容差\n",
    "    )\n",
    "    print(\"✅ 精度对齐通过！\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 对齐失败: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da249e",
   "metadata": {},
   "source": [
    "##### 2D 卷积算子的精度对齐 demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95577a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import numpy as np\n",
    "\n",
    "# 开启 JAX 高精度支持\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# 定义超参数\n",
    "batch, in_channels, out_channels = 2, 3, 4\n",
    "h, w = 10, 10\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1  # PyTorch 的像素填充\n",
    "\n",
    "# 生成随机输入（双精度）\n",
    "x_np = np.random.randn(batch, in_channels, h, w).astype(np.float64)\n",
    "w_np = np.random.randn(out_channels, in_channels, kernel_size, kernel_size).astype(\n",
    "    np.float64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88dad396",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_torch = torch.from_numpy(x_np)\n",
    "w_torch = torch.from_numpy(w_np)\n",
    "\n",
    "# 执行卷积\n",
    "out_torch = torch.nn.functional.conv2d(x_torch, w_torch, stride=stride, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90f9644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 维度对齐：告知 JAX 输入是 NCHW，权重是 OIHW\n",
    "# lhs_spec: (batch, feature, spatial...) [cite: 177]\n",
    "# rhs_spec: (out_feature, in_feature, spatial...) [cite: 178]\n",
    "dn = lax.conv_dimension_numbers(x_np.shape, w_np.shape, (\"NCHW\", \"OIHW\", \"NCHW\"))\n",
    "\n",
    "# 2. 填充对齐：PyTorch 的 padding=1 等价于 JAX 的 [(1, 1), (1, 1)]\n",
    "jax_padding = [(padding, padding), (padding, padding)]\n",
    "\n",
    "# 3. 执行卷积 [cite: 35, 40]\n",
    "# precision 使用 HIGHEST 以确保在所有硬件上对齐 [cite: 376, 377]\n",
    "out_jax = lax.conv_general_dilated(\n",
    "    lhs=jnp.array(x_np),\n",
    "    rhs=jnp.array(w_np),\n",
    "    window_strides=(stride, stride),\n",
    "    padding=jax_padding,\n",
    "    dimension_numbers=dn,\n",
    "    precision=lax.Precision.HIGHEST,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a41b4e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 算子对齐成功！\n",
      "最大绝对误差: 5.329070518200751e-15\n"
     ]
    }
   ],
   "source": [
    "# 转换类型以便比较\n",
    "out_jax_torch = torch.from_numpy(np.array(out_jax))\n",
    "\n",
    "try:\n",
    "    # 核心：不传入 rtol/atol，使用官方默认值\n",
    "    torch.testing.assert_close(out_jax_torch, out_torch)\n",
    "    print(\"✅ 算子对齐成功！\")\n",
    "    print(f\"最大绝对误差: {(out_jax_torch - out_torch).abs().max().item()}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 对齐失败: \\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6be8d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 生成测试数据 (float64 黄金基准)\n",
    "x_np = np.random.randn(1024).astype(np.float64) * 10  # 扩大范围测试稳定性\n",
    "\n",
    "# PyTorch 实现\n",
    "res_pt = F.silu(torch.from_numpy(x_np)).numpy()\n",
    "\n",
    "# JAX 实现\n",
    "res_jax = jnp.array(x_np)\n",
    "# 注：jax.numpy.nn 包含这些激活函数，也可以用数学公式表达\n",
    "res_jax = res_jax * jax.nn.sigmoid(res_jax)\n",
    "res_jax = np.array(res_jax)\n",
    "\n",
    "# 校验\n",
    "torch.testing.assert_close(torch.from_numpy(res_pt), torch.from_numpy(res_jax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b27f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成测试数据\n",
    "x_np = np.random.randn(32, 100).astype(np.float64)\n",
    "\n",
    "# PyTorch 实现\n",
    "# dim=-1 表示在最后一个维度（100）上做归一化\n",
    "res_pt = F.softmax(torch.from_numpy(x_np), dim=-1).numpy()\n",
    "\n",
    "# JAX 实现\n",
    "# axis=-1 对应 PyTorch 的 dim=-1\n",
    "\n",
    "\n",
    "res_jax = jax.nn.softmax(jnp.array(x_np), axis=-1)\n",
    "res_jax = np.array(res_jax)\n",
    "\n",
    "# 校验\n",
    "torch.testing.assert_close(torch.from_numpy(res_pt), torch.from_numpy(res_jax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602f9738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jax.numpy.float8_e4m3fn"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "jnp.float8_e4m3fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767cabc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float8_e4m3fn"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.float8_e4m3fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e437808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jax.numpy.float8_e5m2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.float8_e5m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c77d9719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float8_e5m2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.float8_e5m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4865811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modeltransfer-tdd (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
